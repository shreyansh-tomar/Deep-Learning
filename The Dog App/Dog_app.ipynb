{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dog app.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-L1Oi_YM5ND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import cv2                \n",
        "from glob import glob\n",
        "from io import open\n",
        "import json\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, os.path, random\n",
        "from PIL import Image\n",
        "import requests\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ush4zFGSi8P7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrLYp1j5jKAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqYmPMNUjYlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip dogImages.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc-dhNjwj-cP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip lfw.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh3stzw7kEJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from glob import glob\n",
        "\n",
        "# load filenames for human and dog images\n",
        "human_files = np.array(glob(\"/content/lfw/*/*\"))\n",
        "dog_files = np.array(glob(\"/content/dogImages/*/*/*\"))\n",
        "\n",
        "# print number of images in each dataset\n",
        "print('There are %d total human images.' % len(human_files))\n",
        "print('There are %d total dog images.' % len(dog_files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOyR_JtkkyR5",
        "colab_type": "text"
      },
      "source": [
        "<a id='step1'></a>\n",
        "## Step 1: Detect Humans\n",
        "\n",
        "In this section, we use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  \n",
        "\n",
        "OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.  In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqFbqcZWkX3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2                \n",
        "import matplotlib.pyplot as plt                        \n",
        "%matplotlib inline                               \n",
        "\n",
        "# extract pre-trained face detector\n",
        "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
        "\n",
        "# load color (BGR) image\n",
        "img = cv2.imread(human_files[0])\n",
        "# convert BGR image to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# find faces in image\n",
        "faces = face_cascade.detectMultiScale(gray)\n",
        "\n",
        "# print number of faces detected in the image\n",
        "print('Number of faces detected:', len(faces))\n",
        "\n",
        "# get bounding box for each detected face\n",
        "for (x,y,w,h) in faces:\n",
        "    # add bounding box to color image\n",
        "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)\n",
        "# convert BGR image to RGB for plotting\n",
        "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# display the image, along with bounding box\n",
        "plt.imshow(cv_rgb)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlL6GlSBnKCV",
        "colab_type": "text"
      },
      "source": [
        "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
        "\n",
        "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
        "\n",
        "### Write a Human Face Detector\n",
        "\n",
        "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZToI_sGVlA5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns \"True\" if face is detected in image stored at img_path\n",
        "def face_detector(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    return len(faces) > 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eh0fB4jKszPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "human_files_short = human_files[:100]\n",
        "dog_files_short = dog_files[:100]\n",
        "\n",
        "## Testing the performance of the face_detector algorithm \n",
        "## on the images in human_files_short and dog_files_short.\n",
        "count_human=0\n",
        "count_dog = 0\n",
        "\n",
        "\n",
        "for i in range(len(human_files_short)):\n",
        "    if(face_detector(human_files[i])):\n",
        "        count_human += 1;\n",
        "for i in range(len(dog_files_short)):\n",
        "    if(face_detector(dog_files_short[i])):\n",
        "        count_dog += 1;\n",
        "\n",
        "print(f\"Count of 100 first images humans detected as a human: {count_human}\")\n",
        "print(f\"count of 100 first images dogs detected as a human: {count_dog}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqGaN6DrvtfR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "<a id='step2'></a>\n",
        "## Step 2: Let's Detect Dogs..\n",
        "\n",
        "In this section, we use a [pre-trained model](http://pytorch.org/docs/master/torchvision/models.html) to detect dogs in images.  \n",
        "\n",
        "### Pre-trained VGG-16 Model\n",
        "\n",
        "The code cell below downloads the VGG-16 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "611vmY-PvE_z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "# define VGG16 model\n",
        "VGG16 = models.vgg16(pretrained=True)\n",
        "\n",
        "# check if CUDA is available\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# move model to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    VGG16 = VGG16.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co0RbeB_wjXw",
        "colab_type": "text"
      },
      "source": [
        "###Making Predictions with a Pre-trained Model\n",
        "\n",
        "In the next code cell, we will write a function that accepts a path to an image (such as `'dogImages/train/001.Affenpinscher/Affenpinscher_00001.jpg'`) as input and returns the index corresponding to the ImageNet class that is predicted by the pre-trained VGG-16 model.\n",
        "\n",
        "Before writing the function, let's make sure that we take some time to learn  how to appropriately pre-process tensors for pre-trained models in the [PyTorch documentation](http://pytorch.org/docs/stable/torchvision/models.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1E2uitAv166",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "def load_image(img_path):    \n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    # resize to (244, 244) because VGG16 accept this shape\n",
        "    in_transform = transforms.Compose([\n",
        "                        transforms.Resize(size=(244, 244)),\n",
        "                        transforms.ToTensor()]) # normalizaiton parameters from pytorch doc.\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "def VGG16_predict(img_path):\n",
        "    img = load_image(img_path)\n",
        "    if use_cuda:\n",
        "        img = img.cuda()\n",
        "    ret = VGG16(img)\n",
        "    return torch.max(ret,1)[1].item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olpzotlbzvns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VGG16_predict(dog_files_short[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LheGZ6Et0ErW",
        "colab_type": "text"
      },
      "source": [
        "###Dog Detector\n",
        "\n",
        "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), we will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained VGG-16 model, we need only check if the pre-trained model predicts an index between 151 and 268 (inclusive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiXpkt7EzxeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### returns \"True\" if a dog is detected in the image stored at img_path\n",
        "def dog_detector(img_path):\n",
        "    idx = VGG16_predict(img_path)\n",
        "    return idx >= 151 and idx <= 268"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0SMp6cN0NUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dog_detector_test(files):\n",
        "    detection_cnt = 0;\n",
        "    total_cnt = len(files)\n",
        "    for file in files:\n",
        "        detection_cnt += dog_detector(file)\n",
        "    return detection_cnt, total_cnt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA1zSbkl19So",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"detect a dog in human_files: {} / {}\".format(dog_detector_test(human_files_short)[0], dog_detector_test(human_files_short)[1]))\n",
        "print(\"detect a dog in dog_files: {} / {}\".format(dog_detector_test(dog_files_short)[0], dog_detector_test(dog_files_short)[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhZnW0A-2w5q",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## Let's create a CNN to Classify Dog Breeds (from Scratch)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stOgeKF_1_Vl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "### TODO: Write data loaders for training, validation, and test sets\n",
        "## Specify appropriate transforms, and batch_sizes\n",
        "\n",
        "batch_size = 20\n",
        "num_workers = 0\n",
        "\n",
        "data_dir = '/content/dogImages/'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "valid_dir = os.path.join(data_dir, 'valid/')\n",
        "test_dir = os.path.join(data_dir, 'test/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPFT86Er6nP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "standard_normalization = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97f8WdfI6q1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(1.0, 1.1), shear=5, resample=False, fillcolor=0),\n",
        "        transforms.RandomApply([transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)], p=0.5),\n",
        "        transforms.RandomGrayscale(p=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),    \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1IvUDoN6tGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = datasets.ImageFolder(train_dir, transform=data_transforms['train'])\n",
        "valid_data = datasets.ImageFolder(valid_dir, transform=data_transforms['val'])\n",
        "test_data = datasets.ImageFolder(test_dir, transform=data_transforms['test'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vccrn7Ap6vNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                                           batch_size=batch_size, \n",
        "                                           num_workers=num_workers,\n",
        "                                           shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data,\n",
        "                                           batch_size=batch_size, \n",
        "                                           num_workers=num_workers,\n",
        "                                           shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "                                           batch_size=batch_size, \n",
        "                                           num_workers=num_workers,\n",
        "                                           shuffle=False)\n",
        "loaders_scratch = {\n",
        "    'train': train_loader,\n",
        "    'valid': valid_loader,\n",
        "    'test': test_loader\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Exiick73iF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "num_classes = 133 # total classes of dog breeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bLkNcUe6xd8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    ### TODO: choose an architecture, and complete the class\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        ## Define layers of a CNN\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "\n",
        "        # pool\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # fully-connected\n",
        "        self.fc1 = nn.Linear(7*7*128, 500)\n",
        "        self.fc2 = nn.Linear(500, num_classes)\n",
        "        \n",
        "        # drop-out\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ## Define forward behavior\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(-1, 7*7*128)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "#-#-# You so NOT have to modify the code below this line. #-#-#\n",
        "\n",
        "# instantiate the CNN\n",
        "model_scratch = Net()\n",
        "print(model_scratch)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if use_cuda:\n",
        "    model_scratch.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzhmRYy48ba1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "### TODO: select loss function\n",
        "criterion_scratch = nn.CrossEntropyLoss()\n",
        "\n",
        "### TODO: select optimizer\n",
        "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr = 0.05)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhsUe-yQ9L4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path, last_validation_loss=None):\n",
        "    \"\"\"returns trained model\"\"\"\n",
        "    # initialize tracker for minimum validation loss\n",
        "    if last_validation_loss is not None:\n",
        "        valid_loss_min = last_validation_loss\n",
        "    else:\n",
        "        valid_loss_min = np.Inf\n",
        "    \n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        # initialize variables to monitor training and validation loss\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## find the loss and update the model parameters accordingly\n",
        "            ## record the average training loss, using something like\n",
        "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "\n",
        "            # initialize weights to zero\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            output = model(data)\n",
        "            \n",
        "            # calculate loss\n",
        "            loss = criterion(output, target)\n",
        "            \n",
        "            # back prop\n",
        "            loss.backward()\n",
        "            \n",
        "            # grad\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            \n",
        "            if batch_idx % 100 == 0:\n",
        "                print('Epoch %d, Batch %d loss: %.6f' %\n",
        "                  (epoch, batch_idx + 1, train_loss))\n",
        "            \n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "        model.eval()\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            # move to GPU\n",
        "            if use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            ## update the average validation loss\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "\n",
        "            \n",
        "        # print training/validation statistics \n",
        "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "        \n",
        "        ## TODO: save the model if validation loss has decreased\n",
        "        if valid_loss < valid_loss_min:\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "            valid_loss_min,\n",
        "            valid_loss))\n",
        "            valid_loss_min = valid_loss\n",
        "            \n",
        "    # return trained model\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQktFXNx9RBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "model_scratch = train(20, loaders_scratch, model_scratch, optimizer_scratch, \n",
        "                      criterion_scratch, use_cuda, 'saved_models/model_scratch.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcKb_17G9Ue4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(loaders, model, criterion, use_cuda):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "        # move to GPU\n",
        "        if use_cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# call test function    \n",
        "test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0pyBOdTLEJc",
        "colab_type": "text"
      },
      "source": [
        "### Load Pre-trained Model and Replace Last Fully Connected Layer\n",
        "You can choose from a handful of pre-trained models in PyTorch. The complete list is available [here](https://pytorch.org/docs/stable/torchvision/models.html). I decided to stick with ResNet, since that's what's used in [PyTorch's transfer learning tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#finetuning-the-convnet)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T16NJ01AMf04",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders_transfer = loaders_scratch.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuwxBAEXOICo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "\n",
        "## TODO: Specify model architecture \n",
        "model_transfer = models.resnet50(pretrained=True)\n",
        "\n",
        "    \n",
        "for param in model_transfer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_transfer.fc = nn.Linear(2048, 133, bias=True)\n",
        "fc_parameters = model_transfer.fc.parameters()\n",
        "\n",
        "for param in fc_parameters:\n",
        "    param.requires_grad = True\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    model_transfer = model_transfer.cuda()    \n",
        "\n",
        "model_transfer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeQG0ndyOKHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "criterion_transfer = nn.CrossEntropyLoss()\n",
        "optimizer_transfer = optim.Adam(model_transfer.fc.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkCcSqN4OROJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_time = time.time()\n",
        "\n",
        "model_transfer = train(15, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, 'model_transfer.pt')\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print((end_time - start_time)//60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJFGA4IgOT51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJT3WYw2OYhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def load_input_image(img_path):    \n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    prediction_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
        "                                     transforms.ToTensor(), \n",
        "                                     standard_normalization])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = prediction_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ82lpoKeESw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = [item[4:].replace(\"_\", \" \") for item in loaders_transfer['train'].dataset.classes]\n",
        "\n",
        "\n",
        "def predict_breed_transfer(model, class_names, img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    img = load_input_image(img_path)\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "    idx = torch.argmax(model(img))\n",
        "    return class_names[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkVEixr_eODs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "def run_app(img_path):\n",
        "    ## handle cases for a human face, dog, and neither\n",
        "    img = Image.open(img_path)\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    if dog_detector(img_path) is True:\n",
        "        prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n",
        "        print(\"Dogs Detected!\\nIt looks like a {0}\\n\".format(prediction))  \n",
        "        print(\"--------------------------------------------------------\")\n",
        "    elif face_detector(img_path) > 0:\n",
        "        prediction = predict_breed_transfer(model_transfer, class_names, img_path)\n",
        "        print(\"Hello, human!\\nIf you were a dog..You may look like a {0}\\n\".format(prediction))\n",
        "        print(\"--------------------------------------------------------\")\n",
        "    else:\n",
        "        print(\"Oops! Neither human nor Dog Detected!\")\n",
        "        print(\"--------------------------------------------------------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5oRZliygIEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "humans = ['./new_images/trump.jpg','./new_images/robert.jpg']\n",
        "dogs = ['./new_images/siberian.jpg','./new_images/doberman.jpg']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgzIJdvPgWT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for file in np.hstack((humans[:3], dogs[:3])):\n",
        "    run_app(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yQkxl3QgaWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}