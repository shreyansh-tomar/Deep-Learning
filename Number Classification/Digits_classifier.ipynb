{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST handwritten digit classification problem is a standard dataset used in Computer Vision and Deep Learing Models.\n",
    "Here we will harness the power of Neural Networks to build a deep learning model and train it for recognising __Handwritten Digits__. <br>\n",
    "__NOTE__:The recognised digit is shown as a plot(digits vs probability).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "import helper\n",
    "\n",
    "#Defining a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,),(0.5,)),                    \n",
    "            ])\n",
    "#Downloading and training the dataset\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train= True, transform = transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Backward:\n",
      " None\n",
      "After Backward:\n",
      " tensor([[-3.4123e-04, -3.4123e-04, -3.4123e-04,  ..., -3.4123e-04,\n",
      "         -3.4123e-04, -3.4123e-04],\n",
      "        [ 4.7934e-04,  4.7934e-04,  4.7934e-04,  ...,  4.7934e-04,\n",
      "          4.7934e-04,  4.7934e-04],\n",
      "        [-3.6934e-05, -3.6934e-05, -3.6934e-05,  ..., -3.6934e-05,\n",
      "         -3.6934e-05, -3.6934e-05],\n",
      "        ...,\n",
      "        [-3.2808e-04, -3.2808e-04, -3.2808e-04,  ..., -3.2808e-04,\n",
      "         -3.2808e-04, -3.2808e-04],\n",
      "        [ 1.5916e-03,  1.5916e-03,  1.5916e-03,  ...,  1.5916e-03,\n",
      "          1.5916e-03,  1.5916e-03],\n",
      "        [ 4.8313e-05,  4.8313e-05,  4.8313e-05,  ...,  4.8313e-05,\n",
      "          4.8313e-05,  4.8313e-05]])\n"
     ]
    }
   ],
   "source": [
    "#Building a feed-forward model\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784,128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128,64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64,10),\n",
    "                     nn.LogSoftmax(dim = 1))\n",
    "#Defining Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#Getting the data\n",
    "images, lables = next(iter(trainloader))\n",
    "\n",
    "#Flatening the images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "#forward pass and get our probabilities(because of Softmax use in model)\n",
    "logpbs = model(images)\n",
    "\n",
    "#Calculating loss\n",
    "loss = criterion(logpbs, lables)\n",
    "print('Before Backward:\\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('After Backward:\\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 1.3677e-02,  2.4260e-02, -3.5367e-02,  ..., -1.6010e-02,\n",
      "         -1.3083e-03,  1.0492e-02],\n",
      "        [-9.9794e-03, -7.0698e-03,  1.7925e-02,  ..., -2.9919e-02,\n",
      "          2.4530e-02,  9.7589e-03],\n",
      "        [ 8.0705e-05,  1.8660e-02,  2.0134e-02,  ...,  2.2551e-03,\n",
      "          3.0703e-02, -1.4037e-03],\n",
      "        ...,\n",
      "        [-2.7701e-03, -1.9501e-02, -6.4204e-03,  ..., -3.4450e-02,\n",
      "         -8.9416e-03,  4.0112e-03],\n",
      "        [-1.1003e-04, -3.2806e-02,  1.4117e-02,  ..., -1.5962e-02,\n",
      "         -2.5671e-02,  2.3669e-02],\n",
      "        [-3.0489e-02, -3.0313e-02, -6.8728e-04,  ..., -8.8647e-03,\n",
      "          2.1068e-02,  1.6244e-03]], requires_grad=True)\n",
      "Final weights -  tensor([[ 7.3269e-04,  7.3269e-04,  7.3269e-04,  ...,  7.3269e-04,\n",
      "          7.3269e-04,  7.3269e-04],\n",
      "        [-1.0237e-03, -1.0237e-03, -1.0237e-03,  ..., -1.0237e-03,\n",
      "         -1.0237e-03, -1.0237e-03],\n",
      "        [ 8.2413e-05,  8.2413e-05,  8.2413e-05,  ...,  8.2413e-05,\n",
      "          8.2413e-05,  8.2413e-05],\n",
      "        ...,\n",
      "        [ 2.0261e-04,  2.0261e-04,  2.0261e-04,  ...,  2.0261e-04,\n",
      "          2.0261e-04,  2.0261e-04],\n",
      "        [ 4.1929e-05,  4.1929e-05,  4.1929e-05,  ...,  4.1929e-05,\n",
      "          4.1929e-05,  4.1929e-05],\n",
      "        [ 6.2876e-04,  6.2876e-04,  6.2876e-04,  ...,  6.2876e-04,\n",
      "          6.2876e-04,  6.2876e-04]])\n",
      "Final weights -  Parameter containing:\n",
      "tensor([[ 1.3669e-02,  2.4252e-02, -3.5375e-02,  ..., -1.6017e-02,\n",
      "         -1.3156e-03,  1.0485e-02],\n",
      "        [-9.9691e-03, -7.0595e-03,  1.7935e-02,  ..., -2.9909e-02,\n",
      "          2.4540e-02,  9.7691e-03],\n",
      "        [ 7.9881e-05,  1.8659e-02,  2.0133e-02,  ...,  2.2543e-03,\n",
      "          3.0702e-02, -1.4046e-03],\n",
      "        ...,\n",
      "        [-2.7721e-03, -1.9503e-02, -6.4224e-03,  ..., -3.4452e-02,\n",
      "         -8.9437e-03,  4.0092e-03],\n",
      "        [-1.1045e-04, -3.2806e-02,  1.4116e-02,  ..., -1.5963e-02,\n",
      "         -2.5671e-02,  2.3668e-02],\n",
      "        [-3.0495e-02, -3.0319e-02, -6.9357e-04,  ..., -8.8710e-03,\n",
      "          2.1061e-02,  1.6181e-03]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "logpbs = model(images)\n",
    "loss = criterion(logpbs, labels)\n",
    "loss.backward()\n",
    "print('Final weights - ', model[0].weight.grad)\n",
    "\n",
    "optimizer.step()\n",
    "print('Final weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: , 1.8897874332440179\n",
      "Training loss: , 0.835665123326692\n",
      "Training loss: , 0.5341162552902186\n",
      "Training loss: , 0.4360401889980475\n",
      "Training loss: , 0.3892162507022622\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784,128),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(128,64),\n",
    "                     nn.ReLU(),\n",
    "                     nn.Linear(64,10),\n",
    "                     nn.LogSoftmax(dim = 1))\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.003)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Training loss: , {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
