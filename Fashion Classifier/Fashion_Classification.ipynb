{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Fashion_Classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmN8HNsmWxs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "0fae36cf-01de-4bc5-aef7-96481b715ba0"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:01, 13266903.69it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 94364.81it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 3980172.50it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 32293.50it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting /root/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /root/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTmSLs-wWxtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "        # Dropout module with 0.2 drop probability\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = x.view(x.shape[0], -1)\n",
        "\n",
        "        # Now with dropout\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.dropout(F.relu(self.fc3(x)))\n",
        "\n",
        "        # output so no dropout here\n",
        "        x = F.log_softmax(self.fc4(x), dim=1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwpmTPZhWxtj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "231e7514-06b5-4f35-8b67-b385fc5c977a"
      },
      "source": [
        "model = Classifier()\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "epochs = 30\n",
        "steps = 0\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        log_ps = model(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        \n",
        "        # Turn off gradients for validation, saves memory and computations\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for images, labels in testloader:\n",
        "                log_ps = model(images)\n",
        "                test_loss += criterion(log_ps, labels)\n",
        "                \n",
        "                ps = torch.exp(log_ps)\n",
        "                top_p, top_class = ps.topk(1, dim=1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "\n",
        "        print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "              \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
        "              \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
        "              \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/30..  Training Loss: 0.603..  Test Loss: 0.452..  Test Accuracy: 0.835\n",
            "Epoch: 2/30..  Training Loss: 0.477..  Test Loss: 0.443..  Test Accuracy: 0.844\n",
            "Epoch: 3/30..  Training Loss: 0.449..  Test Loss: 0.402..  Test Accuracy: 0.855\n",
            "Epoch: 4/30..  Training Loss: 0.439..  Test Loss: 0.419..  Test Accuracy: 0.850\n",
            "Epoch: 5/30..  Training Loss: 0.425..  Test Loss: 0.411..  Test Accuracy: 0.855\n",
            "Epoch: 6/30..  Training Loss: 0.407..  Test Loss: 0.385..  Test Accuracy: 0.861\n",
            "Epoch: 7/30..  Training Loss: 0.400..  Test Loss: 0.396..  Test Accuracy: 0.860\n",
            "Epoch: 8/30..  Training Loss: 0.402..  Test Loss: 0.392..  Test Accuracy: 0.864\n",
            "Epoch: 9/30..  Training Loss: 0.393..  Test Loss: 0.401..  Test Accuracy: 0.859\n",
            "Epoch: 10/30..  Training Loss: 0.387..  Test Loss: 0.387..  Test Accuracy: 0.863\n",
            "Epoch: 11/30..  Training Loss: 0.390..  Test Loss: 0.391..  Test Accuracy: 0.862\n",
            "Epoch: 12/30..  Training Loss: 0.387..  Test Loss: 0.381..  Test Accuracy: 0.866\n",
            "Epoch: 13/30..  Training Loss: 0.373..  Test Loss: 0.389..  Test Accuracy: 0.862\n",
            "Epoch: 14/30..  Training Loss: 0.379..  Test Loss: 0.407..  Test Accuracy: 0.862\n",
            "Epoch: 15/30..  Training Loss: 0.372..  Test Loss: 0.392..  Test Accuracy: 0.864\n",
            "Epoch: 16/30..  Training Loss: 0.372..  Test Loss: 0.379..  Test Accuracy: 0.865\n",
            "Epoch: 17/30..  Training Loss: 0.363..  Test Loss: 0.378..  Test Accuracy: 0.871\n",
            "Epoch: 18/30..  Training Loss: 0.370..  Test Loss: 0.389..  Test Accuracy: 0.869\n",
            "Epoch: 19/30..  Training Loss: 0.362..  Test Loss: 0.384..  Test Accuracy: 0.863\n",
            "Epoch: 20/30..  Training Loss: 0.363..  Test Loss: 0.380..  Test Accuracy: 0.869\n",
            "Epoch: 21/30..  Training Loss: 0.363..  Test Loss: 0.373..  Test Accuracy: 0.870\n",
            "Epoch: 22/30..  Training Loss: 0.354..  Test Loss: 0.378..  Test Accuracy: 0.870\n",
            "Epoch: 23/30..  Training Loss: 0.357..  Test Loss: 0.364..  Test Accuracy: 0.877\n",
            "Epoch: 24/30..  Training Loss: 0.353..  Test Loss: 0.384..  Test Accuracy: 0.871\n",
            "Epoch: 25/30..  Training Loss: 0.352..  Test Loss: 0.391..  Test Accuracy: 0.871\n",
            "Epoch: 26/30..  Training Loss: 0.348..  Test Loss: 0.403..  Test Accuracy: 0.866\n",
            "Epoch: 27/30..  Training Loss: 0.345..  Test Loss: 0.376..  Test Accuracy: 0.874\n",
            "Epoch: 28/30..  Training Loss: 0.346..  Test Loss: 0.372..  Test Accuracy: 0.876\n",
            "Epoch: 29/30..  Training Loss: 0.342..  Test Loss: 0.369..  Test Accuracy: 0.874\n",
            "Epoch: 30/30..  Training Loss: 0.337..  Test Loss: 0.378..  Test Accuracy: 0.876\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FljN1AVWWxty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "8f95408c-05ea-4a70-ca3d-4e64da97ee9f"
      },
      "source": [
        "!wget -c https://raw.githubusercontent.com/shreyansh-tomar/Deep-Learning/master/Number%20Classification/helper.py?token=AELHHL5U5QVYDYEGBB6JQQK6CB7WO\n",
        "import helper"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-28 12:07:10--  https://raw.githubusercontent.com/shreyansh-tomar/Deep-Learning/master/Number%20Classification/helper.py?token=AELHHL5U5QVYDYEGBB6JQQK6CB7WO\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2813 (2.7K) [text/plain]\n",
            "Saving to: ‘helper.py?token=AELHHL5U5QVYDYEGBB6JQQK6CB7WO’\n",
            "\n",
            "\r          helper.py   0%[                    ]       0  --.-KB/s               \rhelper.py?token=AEL 100%[===================>]   2.75K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-28 12:07:10 (51.8 MB/s) - ‘helper.py?token=AELHHL5U5QVYDYEGBB6JQQK6CB7WO’ saved [2813/2813]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9plwDlEeZvrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}